{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ca0fdc",
   "metadata": {},
   "source": [
    "# Predictive Asset Health Analytics\n",
    "\n",
    "## Introduction\n",
    "Create an end-to-end predictive asset maintenance solution with XGBoost* from Intel® oneAPI AI Analytics Toolkit (oneAPI). Check out more workflow examples in the [Developer Catalog](https://developer.intel.com/aireferenceimplementations).\n",
    "\n",
    "## Solution Technical Overview\n",
    "\n",
    "Predictive asset maintenance is a method that uses data analysis tools to predict defects and anomalies before they happen. Solutions of huge scale typically require operating across multiple hardware architectures. Accelerating training for the ever-increasing size of datasets and machine learning models is a major challenge while adopting AI (Artificial Intelligence).\n",
    "\n",
    "For an industrial scenario is important to improve the MLOps (Machine Learning Operations) time for developing and deploying new models, this could be challenging due to the ever-increasing size of datasets over a period of time. XGBoost* classifier with HIST tree method addresses this problem improving the overall training/tuning and validation time. A model with a huge set of batch processing requires fast prediction time with a low accuracy lose, daal4py helps the XGBoost* machine learning model to achieve this criteria.\n",
    "\n",
    "For more details, visit the [Predictive Asset Maintenance](https://github.com/intel-innersource/frameworks.ai.platform.sample-apps.predictive-health-analyticsS) GitHub repository.\n",
    "\n",
    "## Validated Hardware Details \n",
    "\n",
    "Intel® oneAPI is used to achieve quick results even when the data for a model are huge. It provides the capability to reuse the code present in different languages so that the hardware utilization is optimized to provide these results.\n",
    "\n",
    "| Recommended Hardware           | Precision  |\n",
    "| ---------------------------- | ---------- |\n",
    "| Intel® 4th Gen Xeon® Scalable Performance processors|BF16 |\n",
    "| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors| FP32 |\n",
    "\n",
    "## How it Works\n",
    "\n",
    "This reference kit generates datasets of given row size for a predictive asset maintenance analytics use-case and stores it in ‘. pkl’ format; these data are then split for training and testing, where we train our model built on the XGBoost* algorithm and predict test data.\n",
    "\n",
    "![Use_case_flow](assets/predictive_asset_maintenance_e2e_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671f658",
   "metadata": {},
   "source": [
    "## Run Using Jupyter Notebook\n",
    "### Run Workflow\n",
    "The following cell provides the variables needed to execute the workflow scripts. \n",
    "If the user did not define previously the path to `WORKSPACE` from console or want to use another `WORKSPACE` location replace `<path_defined_by_user>` with the new path. Before using a new `WORKSPACE` directory make sure that the procedure described in `Get Started` described in the `README.md` file has been followed for the new `WORKSPACE` location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting path variables.\n",
    "import os\n",
    "workspace = os.getenv(\"WORKSPACE\", \"<path_defined_by_user>\")\n",
    "data_dir = workspace+'/data'\n",
    "output_dir = workspace+'/output'\n",
    "print(\"workspace path: {}\".format(workspace))\n",
    "\n",
    "#Setting parameter values.\n",
    "dataset_size = 200000\n",
    "datapkl_path = data_dir+f\"/data_{dataset_size}.pkl\"\n",
    "ncpu = 20\n",
    "tunning = 0 #Hyperparameter tunning, 0 for no tunning\n",
    "data_package = \"pandas\" #Valid options are pandas and modin\n",
    "cross_validation = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f9b30",
   "metadata": {},
   "source": [
    "The following script can be executed with the parameters provided below for generating the test dataset with the active environment.\n",
    "\n",
    "```\n",
    "usage: src/generate_data_pandas.py [-h] [-s SIZE] [-f FILE]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -s SIZE, --size SIZE  data size which is number of rows\n",
    "  -f FILE, --file FILE  output pkl file name\n",
    "  -d, --debug           changes logging level from INFO to DEBUG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda29c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run {workspace}/src/generate_data_pandas.py -s {dataset_size} -f {datapkl_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b4673",
   "metadata": {},
   "source": [
    "Training and prediction along with hyperparameter turning can also be executed independently:\n",
    "```\n",
    "usage: src/train_predict_pam.py [-h] [-f FILE] [-p PACKAGE] [-t TUNING] [-cv CROSS_VALIDATION] [-patch PATCH_SKLEARN]\n",
    "                            -ncpu NUM_CPU\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -f FILE, --file FILE  input pkl file name\n",
    "  -p PACKAGE, --package PACKAGE\n",
    "                        data package to be used (pandas, modin)\n",
    "  -t TUNING, --tuning TUNING\n",
    "                        hyper parameter tuning (0/1)\n",
    "  -cv CROSS_VALIDATION, --cross-validation CROSS_VALIDATION\n",
    "                        cross validation iteration, default 2.\n",
    "  -ncpu NUM_CPU, --num-cpu NUM_CPU\n",
    "                        number of cpu cores, default 4.\n",
    "  -d, --debug           \n",
    "                        changes logging level from INFO to DEBUG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57cb84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run {workspace}/src/train_predict_pam.py -t {tunning} -p {data_package} -f {datapkl_path} -ncpu {ncpu} -cv {cross_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e02eb1",
   "metadata": {},
   "source": [
    "### XGBoost* with oneDAL Python Wrapper (daal4py) model\n",
    "In order to gain even further improved performance on prediction time for the XGBoost* trained machine learning model, it can be converted to a daal4py model. daal4py makes XGBoost* machine learning algorithm execution faster to gain better performance on the underlying hardware by utilizing the Intel® oneAPI Data Analytics Library (oneDAL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577f325",
   "metadata": {},
   "source": [
    "The generated '.pkl' file is used as input for this Python script. \n",
    "```\n",
    "usage: src/daal_xgb_model.py [-h] [-f FILE]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -f FILE, --file FILE  input pkl file name\n",
    "  -d, --debug           changes logging level from INFO to DEBUG\n",
    "```\n",
    "Run the following command to train the model with the given dataset and convert the same to daal4py format and measure the prediction time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run {workspace}/src/daal_xgb_model.py -f {datapkl_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6a6c0",
   "metadata": {},
   "source": [
    "## Expected Output\n",
    "A successful execution of ```generate_data_pandas.py``` should return similar results as shown below:\n",
    "```\n",
    "INFO:__main__:Generating data with the size 800000\n",
    "INFO:__main__:changing Tele_Attatched into an object variable\n",
    "INFO:__main__:Generating our target variable Asset_Label\n",
    "INFO:__main__:Creating correlation between our variables and our target variable\n",
    "INFO:__main__:When age is 60-70 and over 95 change Asset_Label to 1\n",
    "INFO:__main__:When elevation is between 500-1500 change Asset_Label to 1\n",
    "INFO:__main__:When Manufacturer is A, E, or H change Asset_Label to have  95% 0's\n",
    "INFO:__main__:When Species is C2 or C5 change Asset_Label to have 90% to 0's\n",
    "INFO:__main__:When District is NE or W change Asset_Label to have 90% to 0's\n",
    "INFO:__main__:When District is Untreated change Asset_Label to have 70% to 1's\n",
    "INFO:__main__:When Age is greater than 90 and Elevaation is less than 1200 and Original_treatment is Oil change Asset_Label to have 90% to 1's\n",
    "INFO:__main__:=====> Time taken 1.431621 secs for data generation for the size of (800000, 34)\n",
    "INFO:__main__:Saving the data to /localdisk/aagalleg/frameworks.ai.platform.sample-apps.predictive-health-analytics/data/data_800000.pkl ...\n",
    "INFO:__main__:DONE\n",
    "```\n",
    "\n",
    "A successful execution of ```train_predict_pam.py``` should return similar results as shown below:\n",
    "\n",
    "```\n",
    "INFO:__main__:=====> Total Time:\n",
    "6.791231 secs for data size (800000, 34)\n",
    "INFO:__main__:=====> Training Time 3.459683 secs\n",
    "INFO:__main__:=====> Prediction Time 0.281359 secs\n",
    "INFO:__main__:=====> XGBoost accuracy score 0.921640\n",
    "INFO:__main__:DONE\n",
    "```\n",
    "\n",
    "A successful execution of ```train_predict_pam.py``` should return similar results as shown below:\n",
    "\n",
    "```\n",
    "INFO:__main__:Reading the dataset from ./intel_python/data_800000.pkl...\n",
    "INFO:root:sklearn.model_selection.train_test_split: running accelerated version on CPU\n",
    "INFO:root:sklearn.model_selection.train_test_split: running accelerated version on CPU\n",
    "INFO:__main__:XGBoost training time (seconds): 74.001453\n",
    "INFO:__main__:XGBoost inference time (seconds): 0.054897\n",
    "INFO:__main__:DAAL conversion time (seconds): 0.366412\n",
    "INFO:__main__:DAAL inference time (seconds): 0.017998\n",
    "INFO:__main__:XGBoost errors count: 15622\n",
    "INFO:__main__:XGBoost accuracy: 0.921890\n",
    "INFO:__main__:Daal4py errors count: 15622\n",
    "INFO:__main__:Daal4py accuracy: 0.921890\n",
    "INFO:__main__:XGBoost Prediction Time: 0.054897\n",
    "INFO:__main__:daal4py Prediction Time: 0.017998\n",
    "INFO:__main__:daal4py time improvement relative to XGBoost: 0.672158\n",
    "INFO:__main__:Accuracy Difference 0.000000\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:predictive_maintenance_intel] *",
   "language": "python",
   "name": "conda-env-predictive_maintenance_intel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
